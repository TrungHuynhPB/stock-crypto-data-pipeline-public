"""
Prefect subflow to load batch CSVs from MinIO into local TimescaleDB/PostgreSQL.
- Downloads 4 CSV files generated by a1_3_batch_data_to_s3.py:
  * personal transactions
  * corporate transactions
  * customers
  * corporates
- Adds a `source` column = 'BATCH_DATA' for lineage (Kafka streaming will use 'KAFKA_DATA')
- Creates raw tables if not exists
- Converts tables into TimescaleDB hypertables using load_timestamp as time column
- Upserts (ON CONFLICT DO NOTHING) by natural keys

Usage:
  - As a script: python scripts/data_generation/a1_5_batch_s3_to_postgres.py
  - As a subflow: import and call batch_s3_to_postgres_flow(data_date="YYYYMMDD_HHMMSS")

Environment:
  - MinIO access via Prefect S3Bucket block "minio-stock-bucket" (see scripts/utils/minio_connector.py)
  - PostgreSQL (TimescaleDB) via env vars:
      TSDB_HOST (default: timescaledb), POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD
"""

from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import pandas as pd
from psycopg2.extras import execute_values
from prefect import flow, task, get_run_logger
from dotenv import load_dotenv

from scripts.utils.minio_connector import download_file_from_minio
from scripts.utils.db_connector import get_db_connection
from scripts.utils.date_utils import get_canonical_data_date

# Load environment variables
load_dotenv()

# Local temp dir for MinIO downloads
LOCAL_TMP_DIR = Path("data/tmp/minio_downloads")

# S3 folders aligned with a1_3_batch_data_to_s3.py
S3_FOLDER_PERSONAL_TX = "raw-data/transactions/personal"
S3_FOLDER_CORPORATE_TX = "raw-data/transactions/corporate"
S3_FOLDER_CUSTOMERS = "raw-data/customers"
S3_FOLDER_CORPORATES = "raw-data/corporates"
S3_FOLDER_NEWS = "raw-data/crypto_news"

# Target tables (PostgreSQL public schema)
TABLE_TRANSACTIONS_PERSONAL = "raw_transaction_personal"
TABLE_TRANSACTIONS_CORPORATE = "raw_transaction_corporate"
TABLE_CUSTOMERS = "raw_customers"
TABLE_CORPORATES = "raw_corporates"
TABLE_NEWS = "raw_news"

# Columns definitions (order matters for insert)
TRANSACTION_COLUMNS: List[str] = [
    "transaction_id",
    "customer_id",
    "asset_type",
    "asset_symbol",
    "transaction_type",
    "quantity",
    "price_per_unit",
    "transaction_amount",
    "fee_amount",
    "transaction_timestamp",
    "data_date",
    "customer_tier",
    "customer_risk_tolerance",
    "customer_type",
    "data_source",
    "load_timestamp",
    "source",  # 'BATCH_DATA' here
]

CUSTOMER_COLUMNS: List[str] = [
    "customer_id",
    "first_name",
    "last_name",
    "email",
    "gender",
    "age_group",
    "country",
    "registration_date",
    "customer_tier",
    "risk_tolerance",
    "customer_type",
    "company_id",
    "load_timestamp",
    "source",  # 'BATCH_DATA' here
]

CORPORATE_COLUMNS: List[str] = [
    "company_id",
    "company_name",
    "company_type",
    "company_email",
    "country",
    "year_founded",
    "tax_number",
    "office_primary_location",
    "registration_date",
    "load_timestamp",
    "source",  # 'BATCH_DATA' here
]

NEWS_COLUMNS: List[str] = [
    "ticker",
    "asset_type",
    "url",
    "title",
    "description",
    "date",
    "image",
    "load_timestamp",
    "source",  # 'BATCH_DATA' here
]


def get_transactions_ddl(table_name: str) -> str:
    # Hypertable time column is load_timestamp
    return f"""
    CREATE TABLE IF NOT EXISTS public.{table_name} (
        transaction_id VARCHAR(100),
        customer_id VARCHAR(100),
        asset_type VARCHAR(20),
        asset_symbol VARCHAR(50),
        transaction_type VARCHAR(10),
        quantity NUMERIC,
        price_per_unit NUMERIC,
        transaction_amount NUMERIC(20,2),
        fee_amount NUMERIC(20,2),
        transaction_timestamp TIMESTAMP,
        data_date DATE,
        customer_tier VARCHAR(20),
        customer_risk_tolerance VARCHAR(20),
        customer_type VARCHAR(20),
        data_source TEXT,
        load_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
        source VARCHAR(20) NOT NULL DEFAULT 'BATCH_DATA',
        PRIMARY KEY (transaction_id, load_timestamp)
    );
    """


def get_customers_ddl() -> str:
    # Hypertable time column is load_timestamp
    return f"""
    CREATE TABLE IF NOT EXISTS public.{TABLE_CUSTOMERS} (
        customer_id VARCHAR(100),
        first_name VARCHAR(100),
        last_name VARCHAR(100),
        email VARCHAR(255),
        gender VARCHAR(10),
        age_group VARCHAR(20),
        country VARCHAR(50),
        registration_date DATE,
        customer_tier VARCHAR(20),
        risk_tolerance VARCHAR(20),
        customer_type VARCHAR(20),
        company_id VARCHAR(100),
        load_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
        source VARCHAR(20) NOT NULL DEFAULT 'BATCH_DATA',
        PRIMARY KEY (customer_id, load_timestamp)
    );
    """


def get_corporates_ddl() -> str:
    # Hypertable time column is load_timestamp
    return f"""
    CREATE TABLE IF NOT EXISTS public.{TABLE_CORPORATES} (
        company_id VARCHAR(100),
        company_name VARCHAR(255),
        company_type VARCHAR(50),
        company_email VARCHAR(255),
        country VARCHAR(50),
        year_founded INT,
        tax_number VARCHAR(100),
        office_primary_location TEXT,
        registration_date DATE,
        load_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
        source VARCHAR(20) NOT NULL DEFAULT 'BATCH_DATA',
        PRIMARY KEY (company_id, load_timestamp)
    );
    """

def get_news_ddl() -> str:
    # Hypertable time column is load_timestamp
    return f"""
    CREATE TABLE IF NOT EXISTS public.{TABLE_NEWS} (
        ticker VARCHAR(50),
        asset_type VARCHAR(20),
        url TEXT,
        title TEXT,
        description TEXT,
        date TIMESTAMP,
        image TEXT,
        load_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
        source VARCHAR(20) NOT NULL DEFAULT 'BATCH_DATA',
        PRIMARY KEY (ticker, url, load_timestamp)
    );
    """


@task(name="Ensure PostgreSQL Table")
def ensure_table(table_name: str, create_sql: str) -> bool:
    logger = get_run_logger()
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute(create_sql)
            conn.commit()
        logger.info(f"‚úÖ Ensured table exists: {table_name}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed ensuring table {table_name}: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


@task(name="Ensure TimescaleDB Extension")
def ensure_timescaledb_extension() -> bool:
    logger = get_run_logger()
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS timescaledb;")
            conn.commit()
        logger.info("‚úÖ TimescaleDB extension ensured.")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed ensuring TimescaleDB extension: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


@task(name="Ensure composite PK includes load_timestamp")
def ensure_composite_pk_with_time(table_name: str, id_column: str) -> bool:
    """
    Ensure the table's primary key includes load_timestamp (required by TimescaleDB hypertables).
    If an existing PK doesn't include load_timestamp, it will be replaced with a composite PK
    (id_column, load_timestamp). Idempotent.
    """
    logger = get_run_logger()
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # Make sure load_timestamp exists, has default, and is NOT NULL
            cur.execute(f"ALTER TABLE public.{table_name} ALTER COLUMN load_timestamp SET DEFAULT CURRENT_TIMESTAMP;")
            cur.execute(f"ALTER TABLE public.{table_name} ALTER COLUMN load_timestamp SET NOT NULL;")

            # Find existing PK and its columns
            cur.execute(
                """
                SELECT con.conname,
                       array_agg(att.attname ORDER BY att.attnum) AS cols
                FROM pg_constraint con
                JOIN pg_class rel ON rel.oid = con.conrelid
                JOIN pg_namespace nsp ON nsp.oid = rel.relnamespace
                JOIN pg_attribute att ON att.attrelid = rel.oid AND att.attnum = ANY(con.conkey)
                WHERE con.contype = 'p'
                  AND nsp.nspname = 'public'
                  AND rel.relname = %s
                GROUP BY con.conname
                """,
                (table_name,),
            )
            row = cur.fetchone()

            needs_change = False
            drop_constraint = None
            if row is None:
                needs_change = True
            else:
                conname, cols = row[0], row[1]
                cols = list(cols) if cols else []
                if "load_timestamp" not in cols or id_column not in cols or len(cols) != 2:
                    needs_change = True
                    drop_constraint = conname

            if needs_change:
                if drop_constraint:
                    logger.info(f"Dropping existing PK {drop_constraint} on {table_name} to add composite with load_timestamp")
                    cur.execute(f'ALTER TABLE public.{table_name} DROP CONSTRAINT "{drop_constraint}";')
                # Create composite PK including load_timestamp
                cur.execute(
                    f'ALTER TABLE public.{table_name} ADD CONSTRAINT pk_{table_name} PRIMARY KEY ({id_column}, load_timestamp);'
                )

            conn.commit()
        logger.info(f"‚úÖ Composite PK ensured on {table_name} including load_timestamp.")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed ensuring composite PK on {table_name}: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


@task(name="Ensure composite PK includes load_timestamp (multi-col)")
def ensure_composite_pk_with_time_multi(table_name: str, id_columns: List[str]) -> bool:
    """
    Ensure the table's primary key includes load_timestamp with multiple id columns
    (e.g., ticker, url, load_timestamp). Idempotent.
    """
    logger = get_run_logger()
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # Ensure load_timestamp is NOT NULL with default
            cur.execute(f"ALTER TABLE public.{table_name} ALTER COLUMN load_timestamp SET DEFAULT CURRENT_TIMESTAMP;")
            cur.execute(f"ALTER TABLE public.{table_name} ALTER COLUMN load_timestamp SET NOT NULL;")

            # Inspect existing PK
            cur.execute(
                """
                SELECT con.conname,
                       array_agg(att.attname ORDER BY att.attnum) AS cols
                FROM pg_constraint con
                JOIN pg_class rel ON rel.oid = con.conrelid
                JOIN pg_namespace nsp ON nsp.oid = rel.relnamespace
                JOIN pg_attribute att ON att.attrelid = rel.oid AND att.attnum = ANY(con.conkey)
                WHERE con.contype = 'p'
                  AND nsp.nspname = 'public'
                  AND rel.relname = %s
                GROUP BY con.conname
                """,
                (table_name,),
            )
            row = cur.fetchone()

            expected_cols = id_columns + ["load_timestamp"]
            needs_change = False
            drop_constraint = None
            if row is None:
                needs_change = True
            else:
                conname, cols = row[0], row[1]
                cols = list(cols) if cols else []
                if cols != expected_cols:
                    needs_change = True
                    drop_constraint = conname

            if needs_change:
                if drop_constraint:
                    logger.info(f"Dropping existing PK {drop_constraint} on {table_name} to add composite with load_timestamp")
                    cur.execute(f'ALTER TABLE public.{table_name} DROP CONSTRAINT "{drop_constraint}";')
                cols_expr = ", ".join(expected_cols)
                cur.execute(
                    f'ALTER TABLE public.{table_name} ADD CONSTRAINT pk_{table_name} PRIMARY KEY ({cols_expr});'
                )

            conn.commit()
        logger.info(f"‚úÖ Composite PK ensured on {table_name} including load_timestamp with ids {id_columns}.")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed ensuring composite PK on {table_name}: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


@task(name="Ensure Hypertable on load_timestamp")
def ensure_hypertable(table_name: str) -> bool:
    """
    Converts the given table into a TimescaleDB hypertable using load_timestamp as time column.
    Idempotent via if_not_exists => TRUE.
    """
    logger = get_run_logger()
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cur.execute(
                f"SELECT create_hypertable('public.{table_name}', 'load_timestamp', if_not_exists => TRUE, migrate_data => TRUE);"
            )
            # Helpful index for time-ordered queries
            cur.execute(
                f"CREATE INDEX IF NOT EXISTS idx_{table_name}_load_timestamp ON public.{table_name} (load_timestamp DESC);"
            )
            conn.commit()
        logger.info(f"‚úÖ Hypertable ensured on {table_name}(load_timestamp).")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to ensure hypertable for {table_name}: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


@task(name="Insert DataFrame into PostgreSQL")
def insert_df(df: pd.DataFrame, table_name: str, conflict_columns: List[str]) -> bool:
    """
    Inserts DataFrame rows with ON CONFLICT DO NOTHING.
    """
    logger = get_run_logger()
    if df is None or df.empty:
        logger.info(f"‚ö†Ô∏è Empty DataFrame for {table_name}; skipping insert.")
        return True

    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            cols = ",".join(df.columns)
            conflict = ",".join(conflict_columns)
            sql = f"INSERT INTO public.{table_name} ({cols}) VALUES %s ON CONFLICT ({conflict}) DO NOTHING"

            df_prepared = df.where(pd.notnull(df), None)
            data = [tuple(row) for row in df_prepared.to_numpy()]
            execute_values(cur, sql, data)

        conn.commit()
        logger.info(f"‚úÖ Inserted {len(df)} rows into {table_name} (ON CONFLICT DO NOTHING).")
        return True
    except Exception as e:
        logger.error(f"‚ùå Insert failed for {table_name}: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


def build_minio_key_map(data_date: str) -> Dict[str, str]:
    """
    Build MinIO object keys for the expected files based on date suffix.
    """
    return {
        "personal_transactions": f"{S3_FOLDER_PERSONAL_TX}/fake_personal_customers_transactions_{data_date}.csv",
        "corporate_transactions": f"{S3_FOLDER_CORPORATE_TX}/fake_corporate_customers_transactions_{data_date}.csv",
        "customers": f"{S3_FOLDER_CUSTOMERS}/fake_customers_{data_date}.csv",
        "corporates": f"{S3_FOLDER_CORPORATES}/fake_corporates_{data_date}.csv",
        "news": f"{S3_FOLDER_NEWS}/news_raw_{data_date}.csv",
    }


@task(name="Download CSVs from MinIO")
def download_csvs_for_date(data_date: str) -> Dict[str, Optional[Path]]:
    """
    Download the 4 CSVs for a given date. Returns local file paths (if downloaded).
    """
    logger = get_run_logger()
    LOCAL_TMP_DIR.mkdir(parents=True, exist_ok=True)
    out_dir = LOCAL_TMP_DIR / data_date
    out_dir.mkdir(parents=True, exist_ok=True)

    downloaded: Dict[str, Optional[Path]] = {
        "personal_transactions": None,
        "corporate_transactions": None,
        "customers": None,
        "corporates": None,
        "news": None,
    }

    key_map = build_minio_key_map(data_date)
    for k, minio_key in key_map.items():
        local_path = out_dir / Path(minio_key).name
        ok = download_file_from_minio(minio_key, local_path)
        if ok:
            downloaded[k] = local_path
            logger.info(f"üì• Downloaded {k}: {local_path}")
        else:
            logger.warning(f"‚ö†Ô∏è Missing '{k}' file for date {data_date} in MinIO at key: {minio_key}")

    return downloaded


def _add_source_and_cast_dates(df: pd.DataFrame, source_value: str, date_cols: List[str], ts_cols: List[str]) -> pd.DataFrame:
    """
    Add lineage 'source' and robustly cast date/timestamp columns.
    Handles mixed/invalid types by coercing and falling back to safe defaults to avoid .dt errors.
    """
    df = df.copy()
    df["source"] = source_value

    # Handle date-only columns -> store as Python date (YYYY-MM-DD)
    for c in date_cols:
        if c in df.columns:
            try:
                ts = pd.to_datetime(df[c], errors="coerce")
                df[c] = ts.dt.date
            except Exception:
                try:
                    ts = pd.to_datetime(df[c].astype(str), errors="coerce")
                    df[c] = ts.dt.date
                except Exception:
                    # If still failing, null out the column to avoid .dt errors
                    df[c] = None

    # Handle timestamp columns -> store as pandas datetime64[ns]
    for c in ts_cols:
        if c in df.columns:
            try:
                df[c] = pd.to_datetime(df[c], errors="coerce")
            except Exception:
                try:
                    df[c] = pd.to_datetime(df[c].astype(str), errors="coerce")
                except Exception:
                    df[c] = None

    return df


@task(name="Prepare DataFrames")
def prepare_dfs(paths: Dict[str, Optional[Path]]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Reads CSVs, adds source='BATCH_DATA', casts dates/timestamps.
    Returns (personal_transactions_df, corporate_transactions_df, customers_df, corporates_df, news_df)
    """
    logger = get_run_logger()

    def load_tx(path: Optional[Path]) -> pd.DataFrame:
        if not path:
            return pd.DataFrame(columns=TRANSACTION_COLUMNS)
        df = pd.read_csv(path)
        df = _add_source_and_cast_dates(
            df,
            "BATCH_DATA",
            date_cols=["data_date"],
            ts_cols=["transaction_timestamp", "load_timestamp"],
        )
        return df

    personal_tx_df = load_tx(paths.get("personal_transactions"))
    corporate_tx_df = load_tx(paths.get("corporate_transactions"))

    cust_df = pd.DataFrame(columns=CUSTOMER_COLUMNS)
    if paths.get("customers"):
        df = pd.read_csv(paths["customers"])
        df = _add_source_and_cast_dates(
            df,
            "BATCH_DATA",
            date_cols=["registration_date"],
            ts_cols=["load_timestamp"],
        )
        cust_df = df

    corp_df = pd.DataFrame(columns=CORPORATE_COLUMNS)
    if paths.get("corporates"):
        df = pd.read_csv(paths["corporates"])
        df = _add_source_and_cast_dates(
            df,
            "BATCH_DATA",
            date_cols=["registration_date"],
            ts_cols=["load_timestamp"],
        )
        corp_df = df

    news_df = pd.DataFrame(columns=NEWS_COLUMNS)
    if paths.get("news"):
        df = pd.read_csv(paths["news"])
        # Map scraper's 'cryptocurrency' to our schema's 'ticker'
        if "cryptocurrency" in df.columns and "ticker" not in df.columns:
            df = df.rename(columns={"cryptocurrency": "ticker"})
        # Normalize ticker to uppercase strings
        if "ticker" in df.columns:
            df["ticker"] = df["ticker"].astype(str).str.upper()
        # Ensure asset_type present for news (crypto domain)
        if "asset_type" not in df.columns:
            df["asset_type"] = "CRYPTO"
        # Ensure load_timestamp present and non-null so NOT NULL default isn't violated on insert
        if "load_timestamp" not in df.columns or df["load_timestamp"].isna().all():
            df["load_timestamp"] = datetime.utcnow()
        df = _add_source_and_cast_dates(
            df,
            "BATCH_DATA",
            date_cols=["date"],
            ts_cols=["load_timestamp"],
        )
        news_df = df

    # Align columns/order to target schema; fill missing columns
    def align_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        for c in columns:
            if c not in df.columns:
                df[c] = None
        return df[columns]

    if not personal_tx_df.empty:
        personal_tx_df = align_columns(personal_tx_df, TRANSACTION_COLUMNS)
    if not corporate_tx_df.empty:
        corporate_tx_df = align_columns(corporate_tx_df, TRANSACTION_COLUMNS)
    if not cust_df.empty:
        cust_df = align_columns(cust_df, CUSTOMER_COLUMNS)
    if not corp_df.empty:
        corp_df = align_columns(corp_df, CORPORATE_COLUMNS)
    if not news_df.empty:
        news_df = align_columns(news_df, NEWS_COLUMNS)

    logger.info(
        f"Prepared DataFrames: personal_tx={len(personal_tx_df)}, corporate_tx={len(corporate_tx_df)}, customers={len(cust_df)}, corporates={len(corp_df)}, news={len(news_df)}"
    )
    return personal_tx_df, corporate_tx_df, cust_df, corp_df, news_df


@flow(name="5_batch_s3_to_postgres")
def batch_s3_to_postgres_flow(data_date: Optional[str] = None) -> bool:
    """
    Subflow: download CSVs from MinIO for the given date (transactions, customers, corporates, news),
    add source='BATCH_DATA', create tables if not exists, convert to hypertables on load_timestamp,
    and insert into PostgreSQL.

    Args:
        data_date: Canonical date suffix used in filenames (YYYYMMDD_HHMMSS). If None, generate a new one.
    """
    logger = get_run_logger()
    try:
        if data_date is None:
            data_date = get_canonical_data_date()
        logger.info(f"üöÄ Starting Batch S3->PostgreSQL for date {data_date}")

        # Ensure tables exist
        ensure_table(TABLE_TRANSACTIONS_PERSONAL, get_transactions_ddl(TABLE_TRANSACTIONS_PERSONAL))
        ensure_table(TABLE_TRANSACTIONS_CORPORATE, get_transactions_ddl(TABLE_TRANSACTIONS_CORPORATE))
        ensure_table(TABLE_CUSTOMERS, get_customers_ddl())
        ensure_table(TABLE_CORPORATES, get_corporates_ddl())
        ensure_table(TABLE_NEWS, get_news_ddl())

        # Ensure TimescaleDB and hypertables on load_timestamp
        ensure_timescaledb_extension()
        # Ensure composite PKs include load_timestamp (required by TimescaleDB)
        ensure_composite_pk_with_time(TABLE_TRANSACTIONS_PERSONAL, "transaction_id")
        ensure_composite_pk_with_time(TABLE_TRANSACTIONS_CORPORATE, "transaction_id")
        ensure_composite_pk_with_time(TABLE_CUSTOMERS, "customer_id")
        ensure_composite_pk_with_time(TABLE_CORPORATES, "company_id")
        ensure_composite_pk_with_time_multi(TABLE_NEWS, ["ticker", "url"])
        ensure_hypertable(TABLE_TRANSACTIONS_PERSONAL)
        ensure_hypertable(TABLE_TRANSACTIONS_CORPORATE)
        ensure_hypertable(TABLE_CUSTOMERS)
        ensure_hypertable(TABLE_CORPORATES)
        ensure_hypertable(TABLE_NEWS)

        # Download CSVs
        paths = download_csvs_for_date(data_date)

        # Prepare DataFrames
        personal_tx_df, corporate_tx_df, cust_df, corp_df, news_df = prepare_dfs(paths)

        # Insert data (with conflict handling)
        ok_tx_personal = insert_df(personal_tx_df, TABLE_TRANSACTIONS_PERSONAL, conflict_columns=["transaction_id", "load_timestamp"])
        ok_tx_corporate = insert_df(corporate_tx_df, TABLE_TRANSACTIONS_CORPORATE, conflict_columns=["transaction_id", "load_timestamp"])
        ok_cust = insert_df(cust_df, TABLE_CUSTOMERS, conflict_columns=["customer_id", "load_timestamp"])
        ok_corp = insert_df(corp_df, TABLE_CORPORATES, conflict_columns=["company_id", "load_timestamp"])
        ok_news = insert_df(news_df, TABLE_NEWS, conflict_columns=["ticker", "url", "load_timestamp"])

        all_ok = bool(ok_tx_personal and ok_tx_corporate and ok_cust and ok_corp and ok_news)
        if all_ok:
            logger.info("‚úÖ Batch S3->PostgreSQL flow completed successfully.")
        else:
            logger.warning("‚ö†Ô∏è Batch S3->PostgreSQL flow completed with some failures.")
        return all_ok
    except Exception as e:
        logger.error(f"‚ùå Flow failed: {e}")
        raise


if __name__ == "__main__":
    # Run for today's date by default
    batch_s3_to_postgres_flow()
